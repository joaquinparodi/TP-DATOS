{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arbolesejemplo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNV9p4mhXrc4h02LHDSjCXd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaquinparodi/TP1-DATOS/blob/main/arbolesejemplo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5miXzBvNMX-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlOPxhhCNtUT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k73sPRIjqts2"
      },
      "source": [
        "import xgboost as xgb\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn import tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dQmsl6zrPoW"
      },
      "source": [
        "#dataset ejemplo usamos el de la libreria sklearn\r\n",
        "from sklearn.datasets import load_boston\r\n",
        "boston = load_boston()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tNVXuLTrPwM",
        "outputId": "149ed387-03b6-42dc-ef36-ffae6ad750af"
      },
      "source": [
        "#como vemos boston es un diccionario\r\n",
        "print(boston.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBk6rT-dr7ZP",
        "outputId": "18e5f350-076f-4b13-e5fb-92bf92b6f6a2"
      },
      "source": [
        "#aca vemos el tamanio del dataset es de (filas,columnas)\r\n",
        "print(boston.data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqd6eh1urPzE",
        "outputId": "746ce123-1719-4a9a-d1de-8c65cfe08e48"
      },
      "source": [
        "#nombre de los features\r\n",
        "print(boston.feature_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
            " 'B' 'LSTAT']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWh1a0h_rP2c",
        "outputId": "b390bb89-4b49-4845-fb73-f39be9514827"
      },
      "source": [
        "#descripcion del dataset\r\n",
        "print(boston.DESCR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _boston_dataset:\n",
            "\n",
            "Boston house prices dataset\n",
            "---------------------------\n",
            "\n",
            "**Data Set Characteristics:**  \n",
            "\n",
            "    :Number of Instances: 506 \n",
            "\n",
            "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
            "\n",
            "    :Attribute Information (in order):\n",
            "        - CRIM     per capita crime rate by town\n",
            "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
            "        - INDUS    proportion of non-retail business acres per town\n",
            "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
            "        - NOX      nitric oxides concentration (parts per 10 million)\n",
            "        - RM       average number of rooms per dwelling\n",
            "        - AGE      proportion of owner-occupied units built prior to 1940\n",
            "        - DIS      weighted distances to five Boston employment centres\n",
            "        - RAD      index of accessibility to radial highways\n",
            "        - TAX      full-value property-tax rate per $10,000\n",
            "        - PTRATIO  pupil-teacher ratio by town\n",
            "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
            "        - LSTAT    % lower status of the population\n",
            "        - MEDV     Median value of owner-occupied homes in $1000's\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
            "\n",
            "This is a copy of UCI ML housing dataset.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
            "\n",
            "\n",
            "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
            "\n",
            "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
            "prices and the demand for clean air', J. Environ. Economics & Management,\n",
            "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
            "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
            "pages 244-261 of the latter.\n",
            "\n",
            "The Boston house-price data has been used in many machine learning papers that address regression\n",
            "problems.   \n",
            "     \n",
            ".. topic:: References\n",
            "\n",
            "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
            "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gofVDxP8rP5c"
      },
      "source": [
        "#convertimos en un dataframe en pandas\r\n",
        "import pandas as pd\r\n",
        "data=pd.DataFrame(boston.data)\r\n",
        "data.columns=boston.feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "l8Jb--yjrP8k",
        "outputId": "e8fb7fd6-b5c9-496b-cdc1-a9079607d388"
      },
      "source": [
        "#dataframe primeras filas\r\n",
        "data.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.9</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.9</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO      B  LSTAT\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.9   4.98\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.9   9.14\n",
              "\n",
              "[2 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omryThJJrP_8"
      },
      "source": [
        "#agregamos target\r\n",
        "data['PRICE']=boston.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GiRy_uQuPRC",
        "outputId": "7c48211d-c8ed-4df4-a851-2249cdf2b01b"
      },
      "source": [
        "#informacion de los datos del dataframe\r\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 506 entries, 0 to 505\n",
            "Data columns (total 14 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   CRIM     506 non-null    float64\n",
            " 1   ZN       506 non-null    float64\n",
            " 2   INDUS    506 non-null    float64\n",
            " 3   CHAS     506 non-null    float64\n",
            " 4   NOX      506 non-null    float64\n",
            " 5   RM       506 non-null    float64\n",
            " 6   AGE      506 non-null    float64\n",
            " 7   DIS      506 non-null    float64\n",
            " 8   RAD      506 non-null    float64\n",
            " 9   TAX      506 non-null    float64\n",
            " 10  PTRATIO  506 non-null    float64\n",
            " 11  B        506 non-null    float64\n",
            " 12  LSTAT    506 non-null    float64\n",
            " 13  PRICE    506 non-null    float64\n",
            "dtypes: float64(14)\n",
            "memory usage: 55.5 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft2lwQxfustr"
      },
      "source": [
        "X,y=data.iloc[:,:-1], data.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2luhMZ7wus97"
      },
      "source": [
        "#separo el dataframe en set de entranamiento\r\n",
        "#X_train= set entranamiento, y_train el target\r\n",
        "#X_test es el df de test que vamos a usar para predecir\r\n",
        "#y_test es el valor real que vamos a comparar con el valor que\r\n",
        "#nos predice al usar y_train\r\n",
        "  \r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = \\\r\n",
        "    train_test_split(X, y, test_size=0.2, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtLQbbvputDy"
      },
      "source": [
        "#vamos a usar el modelo Random forest para predecir el precio de la\r\n",
        "#propiedad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0dL7dO9utG6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "db8391fe-8bc4-418d-d939-6f934300bcab"
      },
      "source": [
        "#Vamos a ver los hiperparametros para ajustar ese entrenamiento y prediga bien\r\n",
        "\r\n",
        "    n_estimators: cantidad de árboles a construir (100), cada estimadr es un arbol de decision,\r\n",
        "    que se utilizan en paralelo para clasificar un dato.\r\n",
        "    max_depth: máxima profundidad de cada árbol, cuantos niveles podemos irnos\r\n",
        "    en cada uno de los arboles\r\n",
        "    min_samples_split: la cantidad mínima de datos requeridos para splitear un nodo interno (2),\r\n",
        "    esto es para evitar el overffiting, y podar/cortar cuando haiga una minima cantidad de datos\r\n",
        "    en un nodo.no va hacer un split si el nodo tiene dos o menos datos. nos frena para no usar todos\r\n",
        "    los features.\r\n",
        "    min_samples_leaf: cantidad mínima de datos requeridos para ser una hoja (1)\r\n",
        "    max_features: la cantidad de features a considerar cuando se busca el mejor split (n), en cada\r\n",
        "    arbol que tenemos de ramdom forest, tomamos un seleccion aleatoria de features, por defecto son todos\r\n",
        "    los atributos en cada arbol.\r\n",
        "    n_jobs=como tenemos que cada arbol se ejecuta independientemente en el ramdom forest\r\n",
        "    podemos usar las disintas cpu para ejecutar en paralelo cada arbol, para contruir los arboles en paralelo.\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-d33405b6d8d5>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    n_estimators: cantidad de árboles a construir (100), cada estimadr es un arbol de decision,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD0GPED8KWWn",
        "outputId": "bcc207f7-ffef-4664-9ecb-2fdc09d5eede"
      },
      "source": [
        "#Entrenando y prediciendo con hiper-parámetros por defecto\r\n",
        "#en este caso es un modelo regresor, es un problema de regresion.\r\n",
        "rf_model = RandomForestRegressor(random_state=1)#random state 1 para que tengamos el mismo resultado.\r\n",
        "rf_model.fit(X_train, y_train)\r\n",
        "preds = rf_model.predict(X_test)\r\n",
        "#en este caso vamos a usar el error cuadratico medio para calcular el error\r\n",
        "#de la prediccion y  el resultado real. usamos la libreria np. para al raiz, y adentro\r\n",
        "#el metodo error cuadratico medio, entre la prediccion y el valor real\r\n",
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\r\n",
        "print(\"RMSE: %f\" % (rmse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 4.130455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i72guu1EKWer"
      },
      "source": [
        "#feature importance, aveces hay que elegir features de los muchos que tenemos\r\n",
        "#para aumentar la velocidad con que se entrena, y al tener muchos quizas estemos\r\n",
        "#poniendole ruido e empeorando   los resultados "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2amgvI5VgPY"
      },
      "source": [
        "# Gini importance\n",
        "rf_model.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq2ZjWDZWXX6"
      },
      "source": [
        "plt.bar(X_train.columns, rf_model.feature_importances_)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importancia')\n",
        "plt.title('Importancia Features con RF')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9jCcYn-KWpy"
      },
      "source": [
        "#ahroa vamos a ver modificando los hiperparametros si se mejora o emperora el error entre\r\n",
        "#la prediccion y el valor real."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkB0XfoYIsD4"
      },
      "source": [
        "model=RandomForestRegressor(random_state=1, n_estimators=5)\r\n",
        "model.fit(X_train,y_train)\r\n",
        "predictions=model.predict(X_test)\r\n",
        "rmse=np.sqrt(mean_squared_error(y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YokmdROSJp68"
      },
      "source": [
        "print(rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8nxTz0_NMR2"
      },
      "source": [
        "#ahora por cada arbol voy a tener 10 features, acordase que teniamos 13\r\n",
        "model=RandomForestRegressor(random_state=1, n_estimators=5,max_features=10)\r\n",
        "model.fit(X_train,y_train)\r\n",
        "predictions=model.predict(X_test)\r\n",
        "rmse=np.sqrt(mean_squared_error(y_test,predictions))\r\n",
        "print(rmse)\r\n",
        "#observar que dio mejor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFDZP8SFOTi6"
      },
      "source": [
        "#ahora vamos a achicar los arboles a profundidad 2\r\n",
        "model=RandomForestRegressor(random_state=1, n_estimators=5,max_depth=2)\r\n",
        "model.fit(X_train,y_train)\r\n",
        "predictions=model.predict(X_test)\r\n",
        "rmse=np.sqrt(mean_squared_error(y_test,predictions))\r\n",
        "print(rmse)\r\n",
        "#vemos que empeoro porque no hilol fino a mas atributos del split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niEGReFOOy37"
      },
      "source": [
        "plt.bar(X_train.columns, model.feature_importances_)\r\n",
        "plt.xlabel('Features')\r\n",
        "plt.ylabel('Importancia')\r\n",
        "plt.title('Importancia Features con RF')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZGA_HQfP7u_"
      },
      "source": [
        "XgBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH4lRWX-QI8g"
      },
      "source": [
        "learning_rate: tasa de aprendizaje entre 0 y 1, valor chicos, con valores mas grandes overfitting\r\n",
        "max_depth: es bastante chico a comparado del ramdom forest\r\n",
        "subsample: porcentaje de muestras usadas para cada árbol (valor muy bajo, posible underfitting)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqBxpH_nT2N-"
      },
      "source": [
        "colsample_bytree: porcentaje de features usadas para cada árbol (valores muy alto, posible overfitting),porque\r\n",
        "hay que tratar de que cada arbol no vea todos los features y asi evitar el overffiting,0.3 seria una treinta\r\n",
        "por ciento para cada arbol\r\n",
        "n_estimators: cantidad de árboles a construir.\r\n",
        "objective: función de error a utilizar (algunas: reg:squarederror para regresión, reg:logistic o binary:logistic para clasificación)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy_2shcXU0v5"
      },
      "source": [
        "Parámetros de regularización: parte de la regulacion y complejidad del modelo\r\n",
        "\r\n",
        "    gamma: umbral para hacer split basado en la reducción de error de hacer el nuevo split.\r\n",
        "    alpha: regularización para los pesos de las hojas. Un valor más alto genera una mayor regularización.\r\n",
        "    lambda: similar alpha pero para la sintonia fina.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvE4fNUsXRaY"
      },
      "source": [
        "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', \r\n",
        "                colsample_bytree = 0.3, learning_rate = 0.1,\r\n",
        "                max_depth = 5, alpha = 10, n_estimators = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHJ74xVuYWJu"
      },
      "source": [
        "xg_reg.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGnwN0EEYWs2"
      },
      "source": [
        "preds = xg_reg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkP1VexZYW6v"
      },
      "source": [
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\r\n",
        "print(\"RMSE: %f\" % (rmse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA1wq1jTY88Z"
      },
      "source": [
        "#con cienc arboles\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnceGg9jY_Tb"
      },
      "source": [
        "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', \r\n",
        "                colsample_bytree = 0.3, learning_rate = 0.1,\r\n",
        "                max_depth = 5, alpha = 10, n_estimators = 100)\r\n",
        "xg_reg.fit(X_train,y_train)\r\n",
        "preds = xg_reg.predict(X_test)\r\n",
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\r\n",
        "print(\"RMSE: %f\" % (rmse))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2IqrYuZZBft"
      },
      "source": [
        "#con nivel 4, nos da mejor poqrue evitamos un poco el overfitting a \r\n",
        "#cortalo antes el arbol\r\n",
        "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', \r\n",
        "                colsample_bytree = 0.3, learning_rate = 0.1,\r\n",
        "                max_depth = 4, alpha = 10, n_estimators = 100)\r\n",
        "xg_reg.fit(X_train,y_train)\r\n",
        "preds = xg_reg.predict(X_test)\r\n",
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\r\n",
        "print(\"RMSE: %f\" % (rmse))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ddvaGjZBrW"
      },
      "source": [
        "k-fod cross validation para evitar el overffiting\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}